---
---


@string{aps = {American Physical Society,}}

@article{Sim2025AIES,
  abbr              = {AIES},
  title             = {Biased AI Outputs Can Impact Humans' Implicit Bias: A Case Study of the Impact of Gender-Biased Text-to-Image Generators},
  author            = {Mattea Sim and Natalie Grace Brigham and Tadayoshi Kohno and Tessa E. S. Charlesworth and Aylin Caliskan},
  abstract          = {A wave of recent work demonstrates that text-to-image generators (i.e., t2i) can perpetuate and amplify stereotypes about social groups. This research asks: what are the implications of biased t2i for humans who interact with these systems? Across three human-subjects studies, 1,881 participants engaged in a simulated t2i interaction in which the output was controlled to appear either stereotypic, gender-balanced, or counter-stereotypic, via the ratio of perceived women and men in the output of occupation prompts (e.g., a physicist). We then measured people's implicit gender bias using a gender-brilliance implicit association task (IAT), a bias that both relates to stereotypic occupation output in t2i and that has implications for women's representation in different fields. Participants who interacted with neutral t2i output (including only gender-neutral objects, e.g., DVDs) showed relatively high implicit gender-brilliance bias at baseline. Stereotypic t2i output did not increase implicit gender bias relative to this baseline (Study 1). However, participants exposed to counter-stereotypic t2i output had significantly lower implicit gender bias than participants exposed to only gender-neutral output (Studies 1 and 2). Although counter-stereotypic t2i may reduce implicit gender bias amongst users, less than 5% of participants actually preferred the counter-stereotypic representations of women and men. Instead, most participants preferred representations that accurately reflect gender distributions in society or that are more gender-balanced (Study 3). This work demonstrates a novel approach to studying human-AI interaction and reveals important insights for designing generative AI that seeks to mitigate harm. In particular, these findings have implications for understanding the impact of stereotypic t2i on human users, bias mitigation strategies via counter-stereotypic t2i output, and how these impacts (mis)align with people's preferences for t2i representations.},
  journal           = {AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society (AIES)},
  location          = {Madrid},
  volume            = {8},
  issue             = {3},
  pages             = {2375-2386},
  year              = {2025},
  month             = {Oct},
  day               = {15},
  doi               = {10.1609/aies.v8i3.36723},
  url               = {https://doi.org/10.1609/aies.v8i3.36723},
  pdf               = {Sim2025.pdf},
  preview           = {Sim2025Prev.png},
  preview_alt       = {Five AI-generated images of people's faces, with varying appearances.},
  selected          = {true}
},

@article{Gibson2025USENIX,
  abbr              = {USENIX},
  title             = {Analyzing the AI Nudification Application Ecosystem},
  author            = {Cassidy Gibson and Daniel Olszewski and Natalie Grace Brigham and Anna Crowder and Kevin R. B. Butler and Patrick Traynor and Elissa M. Redmiles and Tadayoshi Kohno},
  abstract          = {Given a source image of a clothed person (an image subject), AI-based nudification applications can produce nude (undressed) images of that person. Moreover, not only do such applications exist, but there is ample evidence of the use of such applications in the real world and without the consent of an image subject. Still, despite the growing awareness of the existence of such applications and their potential to violate the rights of image subjects and cause downstream harms, there has been no systematic study of the nudification application ecosystem across multiple applications. We conduct such a study here, focusing on 20 popular and easy-to-find nudification websites. We study the positioning of these web applications (e.g., finding that most sites explicitly target the nudification of women, not all people), the features that they advertise (e.g., ranging from undressing-in-place to the rendering of image subjects in sexual positions, as well as differing user-privacy options), and their underlying monetization infrastructure (e.g., credit cards and cryptocurrencies). We believe this work will empower future, data-informed conversations—within the scientific, technical, and policy communities—on how to better protect individuals' rights and minimize harm in the face of modern (and future) AI-based nudification applications.},
  journal           = {Proceedings of the 34th USENIX Security Symposium},
  location          = {Seattle},
  pages             = {1-20},
  year              = {2025},
  month             = {August},
  arxiv             = {2411.09751},
  url               = {https://www.usenix.org/conference/usenixsecurity25/presentation/gibson},
  pdf               = {Gibson2025.pdf},
  preview           = {Gibson2025Prev.png},
  preview_alt       = {An artistic rendering of a nudification website, showing the options for controling the image outputs.},
  award_name        = {Awards},
  award             = {Internet Defense Prize Runner Up, CSAW 2025 Social Impact Winner},
  selected          = {true}
}

@article{Brigham2024SoLaR,
  abbr              = {SoLaR @ NeurIPS},
  title             = {Developing Story: Case Studies of Generative {AI}{\textquoteright}s Use in Journalism},
  author            = {Natalie Grace Brigham and Chongjiu Gao and Tadayoshi Kohno and Franziska Roesner and Niloofar Mireshghallah},
  abstract          = {Journalists are among the many users of large language models (LLMs). To better understand the journalist-AI interactions, we conduct a study of LLM usage by two news agencies through browsing the WildChat dataset, identifying candidate interactions, and verifying them by matching to online published articles. Our analysis uncovers instances where journalists provide sensitive material such as confidential correspondence with sources or articles from other agencies to the LLM as stimuli and prompt it to generate articles, and publish these machine-generated articles with limited intervention (median output-publication ROUGE-L of 0.62). Based on our findings, we call for further research into what constitutes responsible use of AI, and the establishment of clear guidelines and best practices on using LLMs in a journalistic context.},
  journal           = {Workshop on Socially Responsible Language Modelling Research},
  location          = {Vancouver},
  year              = {2024},
  month             = {December},
  url               = {https://openreview.net/forum?id=415AZy8Jih},
  pdf               = {Brigham2024Developing.pdf},
  arxiv             = {2406.13706},
  preview           = {Brigham2024DevelopingPrev.png},
  preview_alt       = {A circular rainbow pie chart.},
  poster            = {Brigham2023DevelopingPoster.pdf},
  selected          = {true}
 }

@article{Brigham2024SOUPS,
  abbr              = {SOUPS},
  title             = {“Violation of my body:” Perceptions of AI-generated non-consensual (intimate) imagery},
  author            = {Natalie Grace Brigham and Miranda Wei and Tadayoshi Kohno and Elissa M. Redmiles},
  abstract          = {AI technology has enabled the creation of deepfakes: hyper-realistic synthetic media. We surveyed 315 individuals in the U.S. on their views regarding the hypothetical non-consensual creation of deepfakes depicting them, including deepfakes portraying sexual acts. Respondents indicated strong opposition to creating and, even more so, sharing non-consensually created synthetic content, especially if that content depicts a sexual act. However, seeking out such content appeared more acceptable to some respondents. Attitudes around acceptability varied further based on the hypothetical creator’s relationship to the participant, the respondent’s gender and their attitudes towards sexual consent. This study provides initial insight into public perspectives of a growing threat and highlights the need for further research to inform social norms as well as ongoing policy conversations and technical developments in generative AI.},
  journal           = {Proceedings of the Twentieth Symposium on Usable Privacy and Security},
  location          = {Philadelphia},
  pages             = {373--392},
  year              = {2024},
  isbn              = {978-1-939133-42-7},
  month             = {August},
  url               = {https://www.usenix.org/conference/soups2024/presentation/brigham},
  pdf               = {Brigham2024Violation.pdf},
  arxiv             = {2406.05520},
  preview           = {Brigham2024ViolationPrev.png},
  preview_alt       = {Many, primarily red pie charts.},
  slides            = {https://www.usenix.org/system/files/soups2024_slides-brigham.pdf},
  video             = {https://youtu.be/X40-hCuT60c},
  selected          = {true}
 }
